
@article{shrivastava_learning_2016,
	title = {Learning from {Simulated} and {Unsupervised} {Images} through {Adversarial} {Training}},
	url = {https://arxiv.org/abs/1612.07828v2},
	abstract = {With recent progress in graphics, it has become more tractable to train
models on synthetic images, potentially avoiding the need for expensive
annotations. However, learning from synthetic images may not achieve the
desired performance due to a gap between synthetic and real image
distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U)
learning, where the task is to learn a model to improve the realism of a
simulator's output using unlabeled real data, while preserving the annotation
information from the simulator. We develop a method for S+U learning that uses
an adversarial network similar to Generative Adversarial Networks (GANs), but
with synthetic images as inputs instead of random vectors. We make several key
modifications to the standard GAN algorithm to preserve annotations, avoid
artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a
local adversarial loss, and (iii) updating the discriminator using a history of
refined images. We show that this enables generation of highly realistic
images, which we demonstrate both qualitatively and with a user study. We
quantitatively evaluate the generated images by training models for gaze
estimation and hand pose estimation. We show a significant improvement over
using synthetic images, and achieve state-of-the-art results on the MPIIGaze
dataset without any labeled real data.},
	language = {en},
	urldate = {2020-06-05},
	author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
	month = dec,
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\Thomas\\Zotero\\storage\\YXTLMM9G\\Shrivastava et al. - 2016 - Learning from Simulated and Unsupervised Images th.pdf:application/pdf;Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\458NDSBS\\1612.html:text/html}
}

@article{johnson_perceptual_2016,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	url = {http://arxiv.org/abs/1603.08155},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	urldate = {2020-06-05},
	journal = {arXiv:1603.08155 [cs]},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.08155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\3997YU98\\Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\F34V9GMN\\1603.html:text/html}
}

@article{taigman_unsupervised_2016,
	title = {Unsupervised {Cross}-{Domain} {Image} {Generation}},
	url = {http://arxiv.org/abs/1611.02200},
	abstract = {We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.},
	urldate = {2020-06-05},
	journal = {arXiv:1611.02200 [cs]},
	author = {Taigman, Yaniv and Polyak, Adam and Wolf, Lior},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02200},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\FZYIFHS2\\Taigman et al. - 2016 - Unsupervised Cross-Domain Image Generation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\95CGGSFA\\1611.html:text/html}
}

@inproceedings{kalal_forward-backward_2010,
	address = {USA},
	series = {{ICPR} '10},
	title = {Forward-{Backward} {Error}: {Automatic} {Detection} of {Tracking} {Failures}},
	isbn = {978-0-7695-4109-9},
	shorttitle = {Forward-{Backward} {Error}},
	url = {https://doi.org/10.1109/ICPR.2010.675},
	doi = {10.1109/ICPR.2010.675},
	abstract = {This paper proposes a novel method for tracking failure detection. The detection is based on the Forward-Backward error, i.e. the tracking is performed forward and backward in time and the discrepancies between these two trajectories are measured. We demonstrate that the proposed error enables reliable detection of tracking failures and selection of reliable trajectories in video sequences. We demonstrate that the approach is complementary to commonly used normalized cross-correlation (NCC). Based on the error, we propose a novel object tracker called Median Flow. State-of-the-art performance is achieved on challenging benchmark video sequences which include non-rigid objects.},
	urldate = {2020-06-05},
	booktitle = {Proceedings of the 2010 20th {International} {Conference} on {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Kalal, Zdenek and Mikolajczyk, Krystian and Matas, Jiri},
	month = aug,
	year = {2010},
	keywords = {forward backward error, tracking failure detection},
	pages = {2756--2759}
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	language = {en},
	urldate = {2020-06-05},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\BK5W483Q\\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf}
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-06-05},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\X8YFENEK\\Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\B4GBT774\\1412.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2020-06-05},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\IHQ59HSK\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\3XPKFQTT\\1512.html:text/html}
}

@article{ulyanov_instance_2017,
	title = {Instance {Normalization}: {The} {Missing} {Ingredient} for {Fast} {Stylization}},
	shorttitle = {Instance {Normalization}},
	url = {http://arxiv.org/abs/1607.08022},
	abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et al. (2016). We show how a small change in the stylization architecture results in a signiﬁcant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code is available at https://github.com/DmitryUlyanov/texture\_nets. Full paper can be found at https://arxiv.org/abs/1701.02096.},
	language = {en},
	urldate = {2020-06-05},
	journal = {arXiv:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	month = nov,
	year = {2017},
	note = {arXiv: 1607.08022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\GFDAY47H\\Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf:application/pdf}
}

@article{ulyanov_instance_2017-1,
	title = {Instance {Normalization}: {The} {Missing} {Ingredient} for {Fast} {Stylization}},
	shorttitle = {Instance {Normalization}},
	url = {http://arxiv.org/abs/1607.08022},
	abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et al. (2016). We show how a small change in the stylization architecture results in a signiﬁcant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code is available at https://github.com/DmitryUlyanov/texture\_nets. Full paper can be found at https://arxiv.org/abs/1701.02096.},
	language = {en},
	urldate = {2020-06-05},
	journal = {arXiv:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	month = nov,
	year = {2017},
	note = {arXiv: 1607.08022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\IJNB3DYE\\Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf:application/pdf}
}

@article{isola_image--image_2018,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2020-06-05},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/, CVPR 2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\6TSN5MG5\\Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\99FJAPNV\\1611.html:text/html}
}

@article{isola_image--image_2018-1,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	language = {en},
	urldate = {2020-06-05},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/, CVPR 2017},
	file = {Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\NR2LSASR\\Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf}
}

@article{isola_image--image_2018-2,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	language = {en},
	urldate = {2020-06-05},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/, CVPR 2017},
	file = {Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\UNDTRLWN\\Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf}
}

@article{shrivastava_learning_2017,
	title = {Learning from {Simulated} and {Unsupervised} {Images} through {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1612.07828},
	abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator’s output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modiﬁcations to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a ‘self-regularization’ term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of reﬁned images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a signiﬁcant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
	language = {en},
	urldate = {2020-06-05},
	journal = {arXiv:1612.07828 [cs]},
	author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
	month = jul,
	year = {2017},
	note = {arXiv: 1612.07828},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Accepted at CVPR 2017 for oral presentation},
	file = {Shrivastava et al. - 2017 - Learning from Simulated and Unsupervised Images th.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\QC87RDM2\\Shrivastava et al. - 2017 - Learning from Simulated and Unsupervised Images th.pdf:application/pdf}
}

@article{kingma_adam_2017-1,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2020-06-05},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\J4RALTXZ\\Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

@article{isola_image--image_2018-3,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2020-06-05},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/, CVPR 2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\JG9CGDCB\\Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\S4TMYVZ4\\1611.html:text/html}
}

@inproceedings{gatys_image_2016,
	address = {Las Vegas, NV, USA},
	title = {Image {Style} {Transfer} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780634/},
	doi = {10.1109/CVPR.2016.265},
	abstract = {Rendering the semantic content of an image in different styles is a difﬁcult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.},
	language = {en},
	urldate = {2020-06-05},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = jun,
	year = {2016},
	pages = {2414--2423},
	file = {Gatys et al. - 2016 - Image Style Transfer Using Convolutional Neural Ne.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\MMR8BKDI\\Gatys et al. - 2016 - Image Style Transfer Using Convolutional Neural Ne.pdf:application/pdf}
}

@article{rolnick_tackling_2019,
	title = {Tackling {Climate} {Change} with {Machine} {Learning}},
	url = {http://arxiv.org/abs/1906.05433},
	abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
	urldate = {2020-04-08},
	journal = {arXiv:1906.05433 [cs, stat]},
	author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
	month = nov,
	year = {2019},
	note = {arXiv: 1906.05433},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	annote = {Comment: For additional resources, please visit the website that accompanies this paper: https://www.climatechange.ai/},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\S3NS7UVF\\Rolnick et al. - 2019 - Tackling Climate Change with Machine Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\4D9FK2RE\\1906.html:text/html}
}

@article{gulrajani_improved_2017,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	urldate = {2020-03-01},
	journal = {arXiv:1704.00028 [cs, stat]},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	month = dec,
	year = {2017},
	note = {arXiv: 1704.00028},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS camera-ready},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\WER3ZD7F\\Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\JF7FKZW5\\1704.html:text/html}
}

@article{borji_pros_2018,
	title = {Pros and {Cons} of {GAN} {Evaluation} {Measures}},
	url = {http://arxiv.org/abs/1802.03446},
	abstract = {Generative models, in particular generative adversarial networks (GANs), have received significant attention recently. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.},
	urldate = {2020-02-28},
	journal = {arXiv:1802.03446 [cs]},
	author = {Borji, Ali},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.03446},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\E5KC6Y8B\\Borji - 2018 - Pros and Cons of GAN Evaluation Measures.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\EJUXHVUA\\1802.html:text/html}
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
	urldate = {2020-02-27},
	journal = {arXiv:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.00567},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\JZ8I58K2\\Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\L9IPLPCF\\1512.html:text/html}
}

@article{barratt_note_2018,
	title = {A {Note} on the {Inception} {Score}},
	url = {http://arxiv.org/abs/1801.01973},
	abstract = {Deep generative models are powerful tools that have produced impressive results in recent years. These advances have been for the most part empirically driven, making it essential that we use high quality evaluation metrics. In this paper, we provide new insights into the Inception Score, a recently proposed and widely used evaluation metric for generative models, and demonstrate that it fails to provide useful guidance when comparing models. We discuss both suboptimalities of the metric itself and issues with its application. Finally, we call for researchers to be more systematic and careful when evaluating and comparing generative models, as the advancement of the field depends upon it.},
	urldate = {2020-02-27},
	journal = {arXiv:1801.01973 [cs, stat]},
	author = {Barratt, Shane and Sharma, Rishi},
	month = jun,
	year = {2018},
	note = {arXiv: 1801.01973},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Proc. ICML 2018 Workshop on Theoretical Foundations and Applications of Deep Generative Models},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\NUZJV3ZW\\Barratt et Sharma - 2018 - A Note on the Inception Score.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\7Z43M2LJ\\1801.html:text/html}
}

@article{salimans_improved_2016,
	title = {Improved {Techniques} for {Training} {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	urldate = {2020-02-27},
	journal = {arXiv:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03498},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\AHKWUPB7\\Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\U8IRMPAN\\1606.html:text/html}
}

@article{heusel_gans_2018,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	url = {http://arxiv.org/abs/1706.08500},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr{\textbackslash}'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	urldate = {2020-02-27},
	journal = {arXiv:1706.08500 [cs, stat]},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	month = jan,
	year = {2018},
	note = {arXiv: 1706.08500},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Implementations are available at: https://github.com/bioinf-jku/TTUR},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\95GJQ44J\\Heusel et al. - 2018 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\4PDTSG8I\\1706.html:text/html}
}

@article{santurkar_how_nodate,
	title = {How {Does} {Batch} {Normalization} {Help} {Optimization}?},
	abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape signiﬁcantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	language = {en},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Ma, Aleksander},
	pages = {11},
	file = {Santurkar et al. - How Does Batch Normalization Help Optimization.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\55YPJ4LV\\Santurkar et al. - How Does Batch Normalization Help Optimization.pdf:application/pdf}
}

@article{arora_simple_nodate,
	title = {Simple, {Eﬃcient}, and {Neural} {Algorithms} for {Sparse} {Coding}},
	abstract = {Sparse coding is a basic task in many ﬁelds including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Recent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field (1997a) in introducing sparse coding. We also give the ﬁrst eﬃcient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used.},
	language = {en},
	author = {Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Moitra, Ankur},
	pages = {37},
	file = {Arora et al. - Simple, Eﬃcient, and Neural Algorithms for Sparse .pdf:C\:\\Users\\Thomas\\Zotero\\storage\\SLFT74SD\\Arora et al. - Simple, Eﬃcient, and Neural Algorithms for Sparse .pdf:application/pdf}
}

@inproceedings{radford_unsupervised_2015,
	title = {Unsupervised representation learning with deep convolutional generative adversarial networks},
	url = {https://arxiv.org/abs/1511.06434},
	urldate = {2017-09-28},
	booktitle = {{arXiv} preprint {arXiv}:1511.06434},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	year = {2015},
	file = {1511.06434.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\FDNUJIC9\\1511.06434.pdf:application/pdf}
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein gan},
	url = {https://arxiv.org/abs/1701.07875},
	urldate = {2017-09-28},
	journal = {arXiv preprint arXiv:1701.07875},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	year = {2017},
	file = {1701.07875.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\8A5RZY2F\\1701.07875.pdf:application/pdf}
}

@misc{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://ruder.io/optimizing-gradient-descent/index.html},
	author = {Ruder, Sebastian},
	year = {2016}
}

@inproceedings{salimans_improved_2016-1,
	title = {Improved techniques for training gans},
	url = {http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans},
	urldate = {2017-09-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	year = {2016},
	pages = {2234--2242},
	file = {1606.03498.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\X3VX38P6\\1606.03498.pdf:application/pdf}
}

@misc{olah_backpropagation_2015,
	title = {Backpropagation},
	url = {http://colah.github.io/posts/2015-08-Backprop/},
	author = {Olah, Christopher},
	year = {2015}
}

@inproceedings{lecun_efficient_1998,
	address = {London, UK, UK},
	title = {Efficient {BackProp}},
	isbn = {3-540-65311-2},
	url = {http://dl.acm.org/citation.cfm?id=645754.668382},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}, {This} {Book} is an {Outgrowth} of a 1996 {NIPS} {Workshop}},
	publisher = {Springer-Verlag},
	author = {LeCun, Yann and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
	year = {1998},
	pages = {9--50},
	file = {lecun-98b[1].pdf:C\:\\Users\\Thomas\\Zotero\\storage\\HISWVIZD\\lecun-98b[1].pdf:application/pdf}
}

@inproceedings{lecun_gradient-based_1998,
	title = {Gradient-{Based} {Learning} {Applied} to {Document} {Recognition}},
	volume = {86},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
	booktitle = {Proceedings of the {IEEE}},
	author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	year = {1998},
	keywords = {MSc character\_recognition checked mnist network neural},
	pages = {2278--2324},
	file = {lecun-01a[1].pdf:C\:\\Users\\Thomas\\Zotero\\storage\\UASB9JWU\\lecun-01a[1].pdf:application/pdf}
}

@inproceedings{goodfellow_nips_2016,
	title = {{NIPS} 2016 tutorial: {Generative} adversarial networks},
	shorttitle = {{NIPS} 2016 tutorial},
	url = {https://arxiv.org/abs/1701.00160},
	urldate = {2017-09-28},
	booktitle = {{arXiv} preprint {arXiv}:1701.00160},
	author = {Goodfellow, Ian},
	year = {2016},
	file = {1701.00160.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\J7LV73KP\\1701.00160.pdf:application/pdf}
}

@inproceedings{goodfellow_generative_2014-1,
	title = {Generative adversarial nets},
	shorttitle = {Goodfellow\_GAN\_2014},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets},
	urldate = {2017-09-28},
	booktitle = {Advances in neural information processing systems},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
	pages = {2672--2680},
	file = {5423-generative-adversarial-nets.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\7STQ8VAK\\5423-generative-adversarial-nets.pdf:application/pdf}
}

@article{park_adaptive_2019,
	title = {Adaptive {Weighted} {Multi}-{Discriminator} {CycleGAN} for {Underwater} {Image} {Enhancement}},
	volume = {7},
	issn = {2077-1312},
	url = {https://www.mdpi.com/2077-1312/7/7/200},
	doi = {10.3390/jmse7070200},
	abstract = {In this paper, we propose a novel underwater image enhancement method. Typical deep learning models for underwater image enhancement are trained by paired synthetic dataset. Therefore, these models are mostly effective for synthetic image enhancement but less so for real-world images. In contrast, cycle-consistent generative adversarial networks (CycleGAN) can be trained with unpaired dataset. However, performance of the CycleGAN is highly dependent upon the dataset, thus it may generate unrealistic images with less content information than original images. A novel solution we propose here is by starting with a CycleGAN, we add a pair of discriminators to preserve contents of input image while enhancing the image. As a part of the solution, we introduce an adaptive weighting method for limiting losses of the two types of discriminators to balance their inﬂuence and stabilize the training procedure. Extensive experiments demonstrate that the proposed method signiﬁcantly outperforms the state-of-the-art methods on real-world underwater images.},
	language = {en},
	number = {7},
	urldate = {2019-11-13},
	journal = {Journal of Marine Science and Engineering},
	author = {Park, Jaihyun and Han, David K. and Ko, Hanseok},
	month = jun,
	year = {2019},
	pages = {200},
	file = {Park et al. - 2019 - Adaptive Weighted Multi-Discriminator CycleGAN for.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\845GEJ8T\\Park et al. - 2019 - Adaptive Weighted Multi-Discriminator CycleGAN for.pdf:application/pdf}
}

@article{santurkar_how_nodate-1,
	title = {How {Does} {Batch} {Normalization} {Help} {Optimization}?},
	abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape signiﬁcantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	language = {en},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Ma, Aleksander},
	pages = {11},
	file = {Santurkar et al. - How Does Batch Normalization Help Optimization.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\LSBXE5ZW\\Santurkar et al. - How Does Batch Normalization Help Optimization.pdf:application/pdf}
}

@article{zhu_unpaired_2018,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transﬁguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	language = {en},
	urldate = {2019-11-13},
	journal = {arXiv:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: An extended version of our ICCV 2017 paper, v6 updated the implementation details in the appendix. Code and data: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix},
	file = {Zhu et al. - 2018 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\HXXVQI8H\\Zhu et al. - 2018 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf}
}

@article{goldsborough_tour_2016,
	title = {A {Tour} of {TensorFlow}},
	url = {http://arxiv.org/abs/1610.01178},
	abstract = {Deep learning is a branch of artiﬁcial intelligence employing deep neural network architectures that has signiﬁcantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released TensorFlow, an open source deep learning software library for deﬁning, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and ﬁnally comment on observed use-cases of TensorFlow in academia and industry.},
	language = {en},
	urldate = {2020-06-06},
	journal = {arXiv:1610.01178 [cs]},
	author = {Goldsborough, Peter},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.01178},
	keywords = {Computer Science - Machine Learning},
	file = {Goldsborough - 2016 - A Tour of TensorFlow.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\H22WP67C\\Goldsborough - 2016 - A Tour of TensorFlow.pdf:application/pdf}
}

@article{mcculloch_logical_1943,
	title = {A {LOGICAL} {CALCULUS} {OF} {THE} {IDEAS} {IMMANENT} {IN} {NERVOUS} {ACTIVITY}},
	volume = {5},
	language = {en},
	journal = {Bulletin of Mathematical Biophysics},
	author = {Mcculloch, Warren S and Pitts, Walter},
	year = {1943},
	pages = {115--133},
	file = {Mcculloch et Pitts - A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOU.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\Y9EX95LV\\Mcculloch et Pitts - A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOU.pdf:application/pdf}
}

@article{mcculloch_logical_1943-1,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	number = {4},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	year = {1943},
	pages = {115--133}
}

@article{noauthor_notitle_nodate
}

@article{van_der_maaten_visualizing_2008,
	series = {{JMLR}},
	title = {Visualizing data using t-{SNE}},
	url = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf},
	journal = {JMLR 9},
	author = {van der Maaten, Laurens and Hinton, Geoffrey},
	month = aug,
	year = {2008}
}

@book{villani_optimal_2006,
	title = {Optimal {Transport}: {Old} and {New}},
	isbn = {978-3-540-71050-9},
	url = {https://ljk.imag.fr/membres/Emmanuel.Maitre/lib/exe/fetch.php?media=b07.stflour.pdf},
	language = {en},
	author = {Villani, Cédric},
	month = dec,
	year = {2006}
}

@article{radford_unsupervised_2015-1,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {https://arxiv.org/abs/1511.06434v2},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has
seen huge adoption in computer vision applications. Comparatively, unsupervised
learning with CNNs has received less attention. In this work we hope to help
bridge the gap between the success of CNNs for supervised learning and
unsupervised learning. We introduce a class of CNNs called deep convolutional
generative adversarial networks (DCGANs), that have certain architectural
constraints, and demonstrate that they are a strong candidate for unsupervised
learning. Training on various image datasets, we show convincing evidence that
our deep convolutional adversarial pair learns a hierarchy of representations
from object parts to scenes in both the generator and discriminator.
Additionally, we use the learned features for novel tasks - demonstrating their
applicability as general image representations.},
	language = {en},
	urldate = {2020-06-06},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = nov,
	year = {2015},
	file = {Full Text PDF:C\:\\Users\\Thomas\\Zotero\\storage\\3T7YSNCR\\Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\F7J7Z6JA\\1511.html:text/html}
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	language = {en},
	urldate = {2020-06-06},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\MFVTM986\\Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf}
}

@article{long_fully_2014,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {https://arxiv.org/abs/1411.4038v2},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of
features. We show that convolutional networks by themselves, trained
end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic
segmentation. Our key insight is to build "fully convolutional" networks that
take input of arbitrary size and produce correspondingly-sized output with
efficient inference and learning. We define and detail the space of fully
convolutional networks, explain their application to spatially dense prediction
tasks, and draw connections to prior models. We adapt contemporary
classification networks (AlexNet, the VGG net, and GoogLeNet) into fully
convolutional networks and transfer their learned representations by
fine-tuning to the segmentation task. We then define a novel architecture that
combines semantic information from a deep, coarse layer with appearance
information from a shallow, fine layer to produce accurate and detailed
segmentations. Our fully convolutional network achieves state-of-the-art
segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012),
NYUDv2, and SIFT Flow, while inference takes one third of a second for a
typical image.},
	language = {en},
	urldate = {2020-06-06},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = nov,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\Thomas\\Zotero\\storage\\SVVKCEPG\\Long et al. - 2014 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\3X97ZBNB\\1411.html:text/html}
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	language = {en},
	urldate = {2020-06-06},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Added derivations of AdaMax and Nadam},
	file = {Ruder - 2017 - An overview of gradient descent optimization algor.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\SUVS5V2B\\Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf}
}

@article{rosenblatt_perceptron_nodate,
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	author = {Rosenblatt, Frank}
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/h0042519},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {*Brain, *Cognition, *Memory, Nervous System},
	pages = {386--408}
}

@article{misra_mish_2019,
	title = {Mish: {A} {Self} {Regularized} {Non}-{Monotonic} {Neural} {Activation} {Function}},
	journal = {arXiv preprint arXiv:1908.08681},
	author = {Misra, Diganta},
	year = {2019}
}

@inproceedings{godfrey_continuum_2015,
	title = {A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks},
	volume = {1},
	isbn = {989-758-164-2},
	abstract = {SoftExponential},
	publisher = {IEEE},
	author = {Godfrey, Luke B and Gashler, Michael S},
	year = {2015},
	pages = {481--486}
}

@article{srivastava_dropout_nodate,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overﬁtting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	pages = {30},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\UXXLPY5C\\Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}

@misc{verma_understanding_2020,
	title = {Understanding {1D} and {3D} {Convolution} {Neural} {Network} {\textbar} {Keras}},
	url = {https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610},
	abstract = {When we say Convolution Neural Network (CNN), generally we refer to a 2 dimensional CNN which is used for image classification. But there…},
	language = {en},
	urldate = {2020-06-06},
	journal = {Medium},
	author = {Verma, Shiva},
	month = may,
	year = {2020},
	note = {Library Catalog: towardsdatascience.com}
}

@article{dumoulin_guide_2018,
	title = {A guide to convolution arithmetic for deep learning},
	url = {http://arxiv.org/abs/1603.07285},
	abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
	urldate = {2020-06-06},
	journal = {arXiv:1603.07285 [cs, stat]},
	author = {Dumoulin, Vincent and Visin, Francesco},
	month = jan,
	year = {2018},
	note = {arXiv: 1603.07285},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\TGUVSRUR\\Dumoulin et Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\JMZNAD7X\\1603.html:text/html}
}

@article{szegedy_going_2014,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	urldate = {2020-06-06},
	journal = {arXiv:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\GATP7WUS\\Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\9RH4ESDW\\1409.html:text/html}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2020-06-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {NIPS Full Text PDF:C\:\\Users\\Thomas\\Zotero\\storage\\39I4BH4B\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\N325LURC\\4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}

@article{fukushima_neocognitron_1980,
	title = {Neocognitron: {A} self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {0340-1200, 1432-0770},
	shorttitle = {Neocognitron},
	url = {http://link.springer.com/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of selforganization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
	language = {en},
	number = {4},
	urldate = {2020-06-05},
	journal = {Biological Cybernetics},
	author = {Fukushima, Kunihiko},
	month = apr,
	year = {1980},
	pages = {193--202},
	file = {Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\PYKVBNEW\\Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf:application/pdf}
}

@article{benharir_approche_2014,
	title = {Approche {Adaptative} d’une {Commande} {Neuronale} sans capteur d’un {Moteur} {Asynchrone} associée à un {Observateur} par {Mode} {Glissant}},
	abstract = {Cet article présente la commande neuronale d'une machine asynchrone sans capteur de vitesse, afin de remédier aux inconvénients de la commande avec capteur. Une commande neuronale pour la régulation de la vitesse associé à un observateur de type mode glissant a été donné pour estimer le flux rotorique et la vitesse mécanique de la MAS. Cet observateur est associé à une commande vectorielle classique. L'ensemble commande-observateur est testé sur les trajectoires de formes de consignes diverses. Les résultats obtenus par des essais effectués en simulation numérique, pour différents régimes de fonctionnement de l'ensemble onduleur, machine asynchrone et structure de commande, sont présentés ainsi qu'une analyse des performances de la stratégie de commande proposée. Mots clés-machine asynchrone, commande sans capteur, commande neuronale, control adaptative par mode glissant. Abstract-This paper presents the neural network control of a speed sensorless of asynchronous machine in order to overcome the disadvantages of control with sensor. The aim is to verify whether the contribution of artificial intelligence approaches brings significant improvements in terms in precision and robustness of speed estimation. A neural network control for regulating the speed associated with a sliding mode observer has been given to estimate the rotor flux and the mechanical speed of the induction motor. This observer is associated with a conventional vector control. Several tests were performed in simulation through significant operative conditions and have demonstrated the feasibility of the proposed approaches and validate the performance due to the contribution on intelligent techniques.},
	author = {Benharir, N and Zerikat, M and Chekroun, Soufyane and Mechernene, Abdelkader},
	month = jan,
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\Thomas\\Zotero\\storage\\E6BWWRZM\\Benharir et al. - 2014 - Approche Adaptative d’une Commande Neuronale sans .pdf:application/pdf}
}

@misc{noauthor_fig2_nodate,
	title = {Fig.2 {Modèle} du neurone formel de {Mac} {Culloch} et {Pitts} (avec biais) {Il}...},
	url = {https://www.researchgate.net/figure/Modele-du-neurone-formel-de-Mac-Culloch-et-Pitts-avec-biais-Il-sagit-de-realiser_fig1_328996656},
	abstract = {Download scientific diagram {\textbar} Modèle du neurone formel de Mac Culloch et Pitts (avec biais) Il s'agit de réaliser l'apprentissage d'un réseau de neurones identificateur RNI, à partir des séquences des entrées de commande appliquées et des sorties mesurées. La figure 3 illustre le schéma de principe d'une identification directe [8]. from publication: Approche Adaptative d’une Commande Neuronale sans capteur d’un Moteur Asynchrone associée à un Observateur par Mode Glissant {\textbar} Cet article présente la commande neuronale d'une machine asynchrone sans capteur de vitesse, afin de remédier aux inconvénients de la commande avec capteur. Une commande neuronale pour la régulation de la vitesse associé à un observateur de type mode glissant a été donné pour... {\textbar} Engines, Observer and Vector Control {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2020-06-07},
	journal = {ResearchGate},
	note = {Library Catalog: www.researchgate.net},
	file = {Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\X3GN5QQR\\Modele-du-neurone-formel-de-Mac-Culloch-et-Pitts-avec-biais-Il-sagit-de-realiser_fig1_328996656.html:text/html}
}

@article{hadjar_etude_2020,
	title = {Une étude de l'évolutivité des modèles pour la reconnaissance de documents arabes dans un contexte interactif},
	abstract = {Cette thèse aborde la reconnaissance de structures physiques et logiques de documents complexes, riches en variabilité. Plus particulièrement, nous avons étudié l’évolutivité des modèles dans un contexte interactif, où le système intègre progressivement les connaissances induites par les corrections de l’utilisateur. Nous avons étudié les caractéristiques de la langue arabe et nous avons conçu un système de reconnaissance pour cette langue. Dans un premier temps, nous avons adapté des méthodes de segmentation classiques, généralement utilisées pour les documents utilisant un alphabet latin. Nous avons constaté que les résultats obtenus par ces méthodes, peuvent être améliorés en intégrant des connaissances relatives à la classe de documents traitée. Nous préconisons pour cela l’intervention de l’utilisateur. L’idée est de transférer l’expertise de l’utilisateur vers le système de reconnaissance en convertissant ses corrections en connaissances. Ainsi, dans un deuxième temps, nous avons construit deux systèmes de reconnaissance pour traiter respectivement la reconnaissance physique (PLANET) et logique (LUNET) en utilisant un modèle évolutif qui s’adapte à toute nouvelle classe de documents. Le système PLANET utilise plusieurs modèles dédiés, chacun étant associé à une classe de documents donnés. La tâche de ces modèles est d'apprendre les caractéristiques propres à leur classe. Les modèles dédiés sont initialisés avec un modèle général qui est construit en vue d’avoir une connaissance générale de la superclasse de documents. Les systèmes PLANET et LUNET ont été évalués sur les classes de documents bien adaptés à la problématique : les journaux en langue arabe (ANNAHAR, AL HAYAT et AL QUDS). Après le traitement interactif de 10-15 pages de documents, le taux de reconnaissance passe de 96.729\% à 98.687\% ce qui correspond à une diminution du taux d’erreurs de 59.859\%. Quant à LUNET, le taux moyen de reconnaissance est de 94\% avec une diminution du taux d’erreurs de 63.436\%. Ainsi, nous estimons avoir démontré la pertinence d’utiliser des modèles évolutifs pour la reconnaissance de structures physiques et logiques de documents complexes. Ce type d’approche est particulièrement avantageux pour les applications de reconnaissance de taille moyenne ; c’est notamment le cas de la création de fonds de vérité qui est une opération fastidieuse et coûteuse. Grâce à PLANET / LUNET le processus de construction de tels fonds est simplifié. This thesis addresses the recognition of physical and logical structures of complex documents, rich in variability. More precisely, we studied the evolution of models within an interactive context where the system gradually integrates the knowledge induced by the corrections of the user. We studied the features of the Arabic language and we designed a recognition system for this language. In a first stage, we adapted traditional segmentation methods that are generally used for documents using a Latin alphabet. We noted that the results obtained by these methods, can be improved by integrating knowledge related to the treated class of documents. For that purpose we recommend the intervention of a user. The idea is to transfer the expertise from the user towards the recognition system by converting its corrections into knowledge. Thus, in the second stage, we built two systems for performing respectively the physical recognition (PLANET) and logic (LUNET) by using an evolutiv model which adapts to all new class of documents. The PLANET system uses several dedicated models; each one being associated a given class of documents. The task of these models is to learn the specific features of their class. The dedicated models are initialized with a general model, which is built in order to integrate general knowledge of a superclass of documents. The PLANET and LUNET systems have been evaluated on the classes of documents which are well adapted to the problematic: three classes of newspapers in Arabic language (ANNAHAR, AL HAYAT et AL QUDS). After the interactive treatment of 10- 15 pages, the recognition rate raised from 96.729\% to 98.687\% which corresponds to a reduction in the error rate of 59.859\%. As for LUNET, the average recognition rate is 94\% with a reduction in the error rate of 63.436\%. Thus, we estimate having shown the relevance of using evolutiv models for the recognition of the physical and logical structures, of complex documents. This type of approach is particularly advantageous for mid-sized applications; it is for instance the case of ground truth production, which is a tiresome and expensive operation. Thanks to PLANET/LUNET the process of building such ground truth is simplified.},
	author = {Hadjar, Karim},
	month = jun,
	year = {2020}
}

@article{lillicrap_backpropagation_2020,
	title = {Backpropagation and the brain},
	volume = {21},
	issn = {1471-003X, 1471-0048},
	url = {http://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	language = {en},
	number = {6},
	urldate = {2020-06-07},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	pages = {335--346}
}

@book{minsky_perceptrons_2017,
	title = {Perceptrons: {An} introduction to computational geometry},
	isbn = {0-262-53477-0},
	publisher = {MIT press},
	author = {Minsky, Marvin and Papert, Seymour A},
	year = {2017}
}

@article{misra_mish_2019-1,
	title = {Mish: {A} {Self} {Regularized} {Non}-{Monotonic} {Neural} {Activation} {Function}},
	shorttitle = {Mish},
	url = {http://arxiv.org/abs/1908.08681},
	abstract = {The concept of non-linearity in a Neural Network is introduced by an activation function which serves an integral role in the training and performance evaluation of the network. Over the years of theoretical research, many activation functions have been proposed, however, only a few are widely used in mostly all applications which include ReLU (Rectified Linear Unit), TanH (Tan Hyperbolic), Sigmoid, Leaky ReLU and Swish. In this work, a novel neural activation function called as Mish is proposed. The experiments show that Mish tends to work better than both ReLU and Swish along with other standard activation functions in many deep networks across challenging datasets. For instance, in Squeeze Excite Net- 18 for CIFAR 100 classification, the network with Mish had an increase in Top-1 test accuracy by 0.494\% and 1.671\% as compared to the same network with Swish and ReLU respectively. The similarity to Swish along with providing a boost in performance and its simplicity in implementation makes it easier for researchers and developers to use Mish in their Neural Network Models.},
	urldate = {2020-06-07},
	journal = {arXiv:1908.08681 [cs, stat]},
	author = {Misra, Diganta},
	month = oct,
	year = {2019},
	note = {arXiv: 1908.08681},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 13 pages, 26 figures and 6 tables. Draft Version -2},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\R6DVWLAK\\Misra - 2019 - Mish A Self Regularized Non-Monotonic Neural Acti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\WXQFLMSK\\1908.html:text/html}
}

@article{godfrey_continuum_2016,
	title = {A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks},
	url = {http://arxiv.org/abs/1602.01321},
	abstract = {We present the soft exponential activation function for artificial neural networks that continuously interpolates between logarithmic, linear, and exponential functions. This activation function is simple, differentiable, and parameterized so that it can be trained as the rest of the network is trained. We hypothesize that soft exponential has the potential to improve neural network learning, as it can exactly calculate many natural operations that typical neural networks can only approximate, including addition, multiplication, inner product, distance, polynomials, and sinusoids.},
	language = {en},
	urldate = {2020-06-07},
	journal = {arXiv:1602.01321 [cs]},
	author = {Godfrey, Luke B. and Gashler, Michael S.},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.01321},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 6 pages, 8 figures, conference, In Proceedings of Knowledge Discovery and Information Retrieval (KDIR) 2015, Lisbon, Portugal, December 2015},
	file = {Godfrey et Gashler - 2016 - A continuum among logarithmic, linear, and exponen.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\C4SUHPNC\\Godfrey et Gashler - 2016 - A continuum among logarithmic, linear, and exponen.pdf:application/pdf}
}

@misc{jefkine_backpropagation_2016,
	title = {Backpropagation {In} {Convolutional} {Neural} {Networks}},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	abstract = {Backpropagation in convolutional neural networks. A closer look at the concept of weights sharing in convolutional neural networks (CNNs) and an insight on how this affects the forward and backward propagation while computing the gradients during training.},
	language = {en-us},
	urldate = {2020-06-07},
	journal = {DeepGrid},
	author = {Jefkine},
	month = sep,
	year = {2016},
	note = {Library Catalog: www.jefkine.com},
	file = {Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\QX6QTBMC\\backpropagation-in-convolutional-neural-networks.html:text/html}
}

@misc{verma_understanding_2020-1,
	title = {Understanding {1D} and {3D} {Convolution} {Neural} {Network} {\textbar} {Keras}},
	url = {https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610},
	abstract = {When we say Convolution Neural Network (CNN), generally we refer to a 2 dimensional CNN which is used for image classification. But there…},
	language = {en},
	urldate = {2020-06-07},
	journal = {Medium},
	author = {Verma, Shiva},
	month = may,
	year = {2020},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\4ZH269GN\\understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610.html:text/html}
}

@article{rolnick_tackling_2019-1,
	title = {Tackling {Climate} {Change} with {Machine} {Learning}},
	url = {http://arxiv.org/abs/1906.05433},
	abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
	urldate = {2020-06-10},
	journal = {arXiv:1906.05433 [cs, stat]},
	author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
	month = nov,
	year = {2019},
	note = {arXiv: 1906.05433},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	annote = {Comment: For additional resources, please visit the website that accompanies this paper: https://www.climatechange.ai/},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\MYJXETD4\\Rolnick et al. - 2019 - Tackling Climate Change with Machine Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\CS4SY2XA\\1906.html:text/html}
}

@inproceedings{liu_deep_2019,
	title = {Deep video frame interpolation using cyclic frame generation},
	volume = {33},
	isbn = {2374-3468},
	author = {Liu, Yu-Lun and Liao, Yi-Tung and Lin, Yen-Yu and Chuang, Yung-Yu},
	year = {2019},
	pages = {8794--8802},
	file = {Liu et al. - 2019 - Deep video frame interpolation using cyclic frame .pdf:C\:\\Users\\Thomas\\Zotero\\storage\\5TJH8ITW\\Liu et al. - 2019 - Deep video frame interpolation using cyclic frame .pdf:application/pdf}
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	language = {en},
	urldate = {2020-11-16},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
	file = {Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\3L3BKR8F\\1506.01497.pdf:application/pdf}
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efﬁciently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9× faster than R-CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3× faster, tests 10× faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https: //github.com/rbgirshick/fast-rcnn.},
	language = {en},
	urldate = {2020-11-16},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {Girshick - 2015 - Fast R-CNN.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\A7BIUG28\\Girshick - 2015 - Fast R-CNN.pdf:application/pdf}
}

@article{girshick_fast_2015-1,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efﬁciently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9× faster than R-CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3× faster, tests 10× faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https: //github.com/rbgirshick/fast-rcnn.},
	language = {en},
	urldate = {2020-11-16},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
	file = {Girshick - 2015 - Fast R-CNN.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\WAK7Y7VB\\Girshick - 2015 - Fast R-CNN.pdf:application/pdf}
}

@article{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012—achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We ﬁnd that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/˜rbg/rcnn.},
	language = {en},
	urldate = {2020-11-16},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv: 1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
	file = {Girshick et al. - 2014 - Rich feature hierarchies for accurate object detec.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\WIXZGTLJ\\Girshick et al. - 2014 - Rich feature hierarchies for accurate object detec.pdf:application/pdf}
}

@article{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, ﬂexible, and general framework for object instance segmentation. Our approach efﬁciently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, boundingbox object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/ facebookresearch/Detectron.},
	language = {en},
	urldate = {2020-11-16},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: open source; appendix on more results},
	file = {He et al. - 2018 - Mask R-CNN.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\42RNTB4V\\He et al. - 2018 - Mask R-CNN.pdf:application/pdf}
}

@inproceedings{bao_depth-aware_2019,
	title = {Depth-{Aware} {Video} {Frame} {Interpolation}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Bao_Depth-Aware_Video_Frame_Interpolation_CVPR_2019_paper.html},
	urldate = {2020-08-26},
	author = {Bao, Wenbo and Lai, Wei-Sheng and Ma, Chao and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan},
	year = {2019},
	pages = {3703--3712},
	file = {Full Text PDF:C\:\\Users\\Thomas\\Zotero\\storage\\7FKVFC3U\\Bao et al. - 2019 - Depth-Aware Video Frame Interpolation.pdf:application/pdf;Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\7HK4SY3Q\\Bao_Depth-Aware_Video_Frame_Interpolation_CVPR_2019_paper.html:text/html}
}

@article{liu_deep_2019-1,
	title = {Deep {Video} {Frame} {Interpolation} {Using} {Cyclic} {Frame} {Generation}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/4905},
	doi = {10.1609/aaai.v33i01.33018794},
	abstract = {Video frame interpolation algorithms predict intermediate frames to produce videos with higher frame rates and smooth view transitions given two consecutive frames as inputs. We propose that: synthesized frames are more reliable if they can be used to reconstruct the input frames with high quality. Based on this idea, we introduce a new loss term, the cycle consistency loss. The cycle consistency loss can better utilize the training data to not only enhance the interpolation results, but also maintain the performance better with less training data. It can be integrated into any frame interpolation network and trained in an end-to-end manner. In addition to the cycle consistency loss, we propose two extensions: motion linearity loss and edge-guided training. The motion linearity loss approximates the motion between two input frames to be linear and regularizes the training. By applying edge-guided training, we further improve results by integrating edge information into training. Both qualitative and quantitative experiments demonstrate that our model outperforms the state-of-the-art methods. The source codes of the proposed method and more experimental results will be available at https://github.com/alex04072000/CyclicGen.},
	language = {en},
	number = {01},
	urldate = {2020-08-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Yu-Lun and Liao, Yi-Tung and Lin, Yen-Yu and Chuang, Yung-Yu},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {8794--8802},
	file = {Full Text PDF:C\:\\Users\\Thomas\\Zotero\\storage\\WY94H4EW\\Liu et al. - 2019 - Deep Video Frame Interpolation Using Cyclic Frame .pdf:application/pdf;Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\JTP9G7GZ\\4905.html:text/html}
}

@inproceedings{meyer_phasenet_2018,
	title = {{PhaseNet} for {Video} {Frame} {Interpolation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Meyer_PhaseNet_for_Video_CVPR_2018_paper.html},
	urldate = {2020-08-26},
	author = {Meyer, Simone and Djelouah, Abdelaziz and McWilliams, Brian and Sorkine-Hornung, Alexander and Gross, Markus and Schroers, Christopher},
	year = {2018},
	pages = {498--507},
	file = {Full Text PDF:C\:\\Users\\Thomas\\Zotero\\storage\\FLD6D5NB\\Meyer et al. - 2018 - PhaseNet for Video Frame Interpolation.pdf:application/pdf;Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\3S5RW448\\Meyer_PhaseNet_for_Video_CVPR_2018_paper.html:text/html}
}

@inproceedings{niklaus_context-aware_2018,
	title = {Context-{Aware} {Synthesis} for {Video} {Frame} {Interpolation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Niklaus_Context-Aware_Synthesis_for_CVPR_2018_paper.html},
	urldate = {2020-08-26},
	author = {Niklaus, Simon and Liu, Feng},
	year = {2018},
	pages = {1701--1710},
	file = {Full Text PDF:C\:\\Users\\Thomas\\Zotero\\storage\\7U2DGT6R\\Niklaus et Liu - 2018 - Context-Aware Synthesis for Video Frame Interpolat.pdf:application/pdf;Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\XNB48GQ2\\Niklaus_Context-Aware_Synthesis_for_CVPR_2018_paper.html:text/html}
}

@article{niklaus_video_2017,
	title = {Video {Frame} {Interpolation} via {Adaptive} {Separable} {Convolution}},
	url = {http://arxiv.org/abs/1708.01692},
	abstract = {Standard video frame interpolation methods ﬁrst estimate optical ﬂow between input frames and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps into a single convolution process by convolving input frames with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over input frames using pairs of 1D kernels. Compared to regular 2D kernels, the 1D kernels require signiﬁcantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two input frames and estimates pairs of 1D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation.},
	language = {en},
	urldate = {2020-08-26},
	journal = {arXiv:1708.01692 [cs]},
	author = {Niklaus, Simon and Mai, Long and Liu, Feng},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.01692},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV 2017, http://graphics.cs.pdx.edu/project/sepconv/},
	file = {Niklaus et al. - 2017 - Video Frame Interpolation via Adaptive Separable C.pdf:C\:\\Users\\Thomas\\Zotero\\storage\\8AFUBCT6\\Niklaus et al. - 2017 - Video Frame Interpolation via Adaptive Separable C.pdf:application/pdf}
}

@article{jiang_super_2018,
	title = {Super {SloMo}: {High} {Quality} {Estimation} of {Multiple} {Intermediate} {Frames} for {Video} {Interpolation}},
	shorttitle = {Super {SloMo}},
	url = {http://arxiv.org/abs/1712.00080},
	abstract = {Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. We use 1,132 video clips with 240-fps, containing 300K individual video frames, to train our network. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.},
	urldate = {2020-08-26},
	journal = {arXiv:1712.00080 [cs]},
	author = {Jiang, Huaizu and Sun, Deqing and Jampani, Varun and Yang, Ming-Hsuan and Learned-Miller, Erik and Kautz, Jan},
	month = jul,
	year = {2018},
	note = {arXiv: 1712.00080
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2018 version with minor revisions and supplementary material included. Project page http://jianghz.me/projects/superslomo},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\ZM9TKQI9\\Jiang et al. - 2018 - Super SloMo High Quality Estimation of Multiple I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\5GJRPIRT\\1712.html:text/html}
}

@article{van_amersfoort_frame_2019,
	title = {Frame {Interpolation} with {Multi}-{Scale} {Deep} {Loss} {Functions} and {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1711.06045},
	abstract = {Frame interpolation attempts to synthesise frames given one or more consecutive video frames. In recent years, deep learning approaches, and notably convolutional neural networks, have succeeded at tackling low- and high-level computer vision problems including frame interpolation. These techniques often tackle two problems, namely algorithm efficiency and reconstruction quality. In this paper, we present a multi-scale generative adversarial network for frame interpolation ({\textbackslash}mbox\{FIGAN\}). To maximise the efficiency of our network, we propose a novel multi-scale residual estimation module where the predicted flow and synthesised frame are constructed in a coarse-to-fine fashion. To improve the quality of synthesised intermediate video frames, our network is jointly supervised at different levels with a perceptual loss function that consists of an adversarial and two content losses. We evaluate the proposed approach using a collection of 60fps videos from YouTube-8m. Our results improve the state-of-the-art accuracy and provide subjective visual quality comparable to the best performing interpolation method at x47 faster runtime.},
	urldate = {2020-08-26},
	journal = {arXiv:1711.06045 [cs]},
	author = {van Amersfoort, Joost and Shi, Wenzhe and Acosta, Alejandro and Massa, Francisco and Totz, Johannes and Wang, Zehan and Caballero, Jose},
	month = feb,
	year = {2019},
	note = {arXiv: 1711.06045},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\WR3FIC46\\van Amersfoort et al. - 2019 - Frame Interpolation with Multi-Scale Deep Loss Fun.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\B899H9PZ\\1711.html:text/html}
}

@article{niklaus_video_2017-1,
	title = {Video {Frame} {Interpolation} via {Adaptive} {Convolution}},
	url = {http://arxiv.org/abs/1703.07514},
	abstract = {Video frame interpolation typically involves two steps: motion estimation and pixel synthesis. Such a two-step approach heavily depends on the quality of motion estimation. This paper presents a robust video frame interpolation method that combines these two steps into a single process. Specifically, our method considers pixel synthesis for the interpolated frame as local convolution over two input frames. The convolution kernel captures both the local motion between the input frames and the coefficients for pixel synthesis. Our method employs a deep fully convolutional neural network to estimate a spatially-adaptive convolution kernel for each pixel. This deep neural network can be directly trained end to end using widely available video data without any difficult-to-obtain ground-truth data like optical flow. Our experiments show that the formulation of video interpolation as a single convolution process allows our method to gracefully handle challenges like occlusion, blur, and abrupt brightness change and enables high-quality video frame interpolation.},
	urldate = {2020-08-26},
	journal = {arXiv:1703.07514 [cs]},
	author = {Niklaus, Simon and Mai, Long and Liu, Feng},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.07514},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017, http://graphics.cs.pdx.edu/project/adaconv},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\ZW9S3E3R\\Niklaus et al. - 2017 - Video Frame Interpolation via Adaptive Convolution.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\62M3FR8A\\1703.html:text/html}
}

@inproceedings{park_deep_2019,
	title = {Deep {Learning} {Approach} to {Video} {Frame} {Rate} {Up}-{Conversion} {Using} {Bilateral} {Motion} {Estimation}},
	doi = {10.1109/APSIPAASC47483.2019.9023270},
	abstract = {We propose a deep learning-based frame rate upconversion algorithm using bilateral motion estimation. We first estimate bilateral motion fields by employing a convolutional neural network. Also, we approximate intermediate bi-directional motion fields, assuming linear motions between successive frames. Finally, we develop the synthesis network to produce an intermediate frame by merging the warped frames, which are obtained using the two kinds of motion fields. Experimental results demonstrate that the proposed algorithm generates high-quality intermediate frames on challenging sequences with large motions and occlusion, and outperforms state-of-the-art conventional algorithms.},
	booktitle = {2019 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Park, Junheum and Lee, Chul and Kim, Chang-Su},
	month = nov,
	year = {2019},
	note = {ISSN: 2640-0103},
	keywords = {Approximation algorithms, Bidirectional control, bilateral motion estimation, bilateral motion fields, convolutional neural network, Convolutional neural networks, deep learning-based frame rate upconversion algorithm, image sequences, Interpolation, learning (artificial intelligence), Machine learning, motion estimation, Motion estimation, neural nets, video frame rate up-conversion, video signal processing, Visualization},
	pages = {1970--1975},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Thomas\\Zotero\\storage\\HBHSA34P\\9023270.html:text/html}
}

@article{noauthor_notitle_nodate-1
}

@article{liu_image_2018,
	title = {Image {Inpainting} for {Irregular} {Holes} {Using} {Partial} {Convolutions}},
	url = {http://arxiv.org/abs/1804.07723},
	abstract = {Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach.},
	urldate = {2021-06-03},
	journal = {arXiv:1804.07723 [cs]},
	author = {Liu, Guilin and Reda, Fitsum A. and Shih, Kevin J. and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
	month = dec,
	year = {2018},
	note = {arXiv: 1804.07723},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Update: camera-ready; L1 loss is size-averaged; code of partial conv layer: https://github.com/NVIDIA/partialconv. Published at ECCV 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Thomas\\Zotero\\storage\\FQ3ABTHJ\\Liu et al. - 2018 - Image Inpainting for Irregular Holes Using Partial.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Thomas\\Zotero\\storage\\YV3DV8UJ\\1804.html:text/html}
}
